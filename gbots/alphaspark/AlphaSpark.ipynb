{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mIqDFrFlhgzX"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras.layers import *\n",
    "from keras.initializers import RandomUniform\n",
    "from keras.utils import plot_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nS7H6nyq6Yq4"
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "np.set_printoptions(threshold=np.inf)\n",
    "\n",
    "# construct entity list:\n",
    "MAX_NUM_ENTITIES = 256\n",
    "EMBEDDED_ENTITY_LENGTH = 74\n",
    "MISSING_BIAS = -1\n",
    "MAX_COORD_VALUE_GAME_UNITS = 256\n",
    "# NUM_SPATIAL_BUCKETS = 64\n",
    "\n",
    "def get_one_hot(targets, nb_classes):\n",
    "    res = np.eye(nb_classes)[np.array(targets).reshape(-1)]\n",
    "    return res.reshape(list(targets.shape)+[nb_classes]).astype(int)\n",
    "\n",
    "def prodToRadius(prod):\n",
    "    return (prod+70)*12/85 \n",
    "\n",
    "def encodeSpatialCoord(data):\n",
    "    gameUnitCompressionFactor = 40\n",
    "    dataToUintRange = 255 * (data.astype(int) / (2 * MAX_COORD_VALUE_GAME_UNITS) + 0.5)\n",
    "    inRangeData = np.clip(dataToUintRange, 0, 255)\n",
    "    return np.unpackbits(inRangeData.astype(np.uint8)).reshape(-1, 8)\n",
    "\n",
    "def replaceNoneWith(array, replacement):\n",
    "    return np.where(array == None, replacement, array)\n",
    "\n",
    "FRIENDLY_ENCODING_POSITION = 32 + 2\n",
    "ENEMY_ENCODING_POSITION = FRIENDLY_ENCODING_POSITION + 1\n",
    "\n",
    "def getEncodedEmbedding(rawData, playerN):\n",
    "    maxShips = 200 # sqrt(max)+1 = 15\n",
    "    maxProduction = 100\n",
    "    minProduction = 15\n",
    "    maxRadius = 100 # for very spread out fleets\n",
    "\n",
    "    unitType = get_one_hot((rawData[:, 0] == 'fleet').astype(int), \n",
    "                           2)\n",
    "    # row 2 is player, row 3 enemy, row 4 neutral\n",
    "    friendly = get_one_hot((rawData[:, 1].astype(int) != playerN).astype(int) + (rawData[:, 2]).astype(int), \n",
    "                           3)\n",
    "    # TODO: this is a bit granular...\n",
    "    ships = get_one_hot(np.sqrt(np.minimum((rawData[:, 3]).astype(int), maxShips)).astype(int),\n",
    "                        int(np.sqrt(maxShips) + 1) )\n",
    "    prodCleaned = replaceNoneWith(rawData[:, 4], 0).astype(int)\n",
    "    production = get_one_hot(np.sqrt(np.minimum(prodCleaned, maxProduction)).astype(int),\n",
    "                             int(np.sqrt(maxProduction) + 1) )\n",
    "    radius = get_one_hot(np.sqrt(np.minimum(rawData[:, 5].astype(int), maxRadius)).astype(int),\n",
    "                        int(np.sqrt(maxRadius) + 1) )\n",
    "    x = encodeSpatialCoord(rawData[:, 6])\n",
    "    y = encodeSpatialCoord(rawData[:, 7])\n",
    "    \n",
    "    targetIndices = rawData[:, 8]\n",
    "    # 0-indexed, 3 players (incl. neutral)\n",
    "    targetIndices[targetIndices != None] = targetIndices[targetIndices != None] - 4\n",
    "\n",
    "    targetX = np.copy(targetIndices)\n",
    "    # If fleet, then get X of target. Otherwise, get X of the planet.\n",
    "    targetX[targetX != None] = rawData[targetX[targetX != None].astype(int), 6]\n",
    "    targetX[targetX == None] = rawData[targetX == None, 6]\n",
    "    targetX = encodeSpatialCoord(targetX)\n",
    "\n",
    "    targetY = np.copy(targetIndices)\n",
    "    # If fleet, then get Y of target. Otherwise, get Y of the planet.\n",
    "    targetY[targetY != None] = rawData[targetY[targetY != None].astype(int), 7]\n",
    "    targetY[targetY == None] = rawData[targetY == None, 7]\n",
    "    targetY = encodeSpatialCoord(targetY)\n",
    "\n",
    "    finalEmbedding = np.concatenate((x, y, targetX, targetY, unitType, friendly, ships, production, radius), axis=1)\n",
    "    return finalEmbedding\n",
    "\n",
    "\n",
    "# take preprocessed entities, then add -1e9 biases to any entries to create 512 length array\n",
    "# TODO: They say their bias is -1e9????????\n",
    "def getEntityEmbedding(frame, playerN):\n",
    "     # 3 for users\n",
    "    entities = [e for i, e in enumerate(frame['items']) if e['type'] != 'user' and i < MAX_NUM_ENTITIES + 3]\n",
    "    rawData = np.array([[e.get(j) for j in ['type', 'owner', 'neutral', 's', 'p', 'r', 'x', 'y', 'target']] \n",
    "                          for e in entities])\n",
    "    embedding = getEncodedEmbedding(rawData, int(playerN))\n",
    "    amountToPad = MAX_NUM_ENTITIES - len(embedding)\n",
    "    return np.pad(embedding, pad_width=((0,amountToPad), (0,0)), mode='constant', constant_values=MISSING_BIAS)\n",
    "\n",
    "def getGameEntityEmbeddings(game):\n",
    "    return np.array([getEntityEmbedding(frame, game['playerN']) for frame in game['frames']])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json \n",
    "# import numpy as np\n",
    "\n",
    "# with open('D:\\GalconZero\\Waffle\\\\train_test_SMALL.jsonl') as f:\n",
    "#     for line in f:\n",
    "#         testGame = json.loads(line)\n",
    "#         testEntityEmbeddings = getGameEntityEmbeddings(testGame)\n",
    "#         testX = testEntityEmbeddings\n",
    "#         testY = getTrainYForGame(testGame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rotateVec(angle, xs, ys):\n",
    "    theSin = np.sin(np.deg2rad(angle))\n",
    "    theCos = np.cos(np.deg2rad(angle))\n",
    "    newXs = theCos * xs - theSin * ys\n",
    "    newYs = theSin * xs + theCos * ys\n",
    "    return newXs, newYs\n",
    "\n",
    "# augment training data by reflecting about X axis\n",
    "def augmentDataMirror(original):\n",
    "    data = original.copy()\n",
    "    for frame_i, frameEntities in enumerate(original):\n",
    "        mask = np.sum(frameEntities[...,0:8], -1) >= 0\n",
    "    \n",
    "        newXs = 255 - np.packbits(frameEntities[mask,0:8], -1)\n",
    "        data[frame_i,mask,0:8] = np.unpackbits(newXs, -1)\n",
    "        \n",
    "        newTargetXs = 255 - np.packbits(frameEntities[mask,16:24], -1)\n",
    "        data[frame_i,mask,16:24] = np.unpackbits(newTargetXs, -1)\n",
    "\n",
    "    return data\n",
    "    \n",
    "# augment training data by rotating about origin\n",
    "def augmentDataRotate(original, angleDegrees):\n",
    "    data = original.copy()\n",
    "    for frame_i, frameEntities in enumerate(original):\n",
    "        mask = np.sum(frameEntities[...,0:8], -1) >= 0\n",
    "        \n",
    "        entityXs = np.packbits(frameEntities[mask,0:8], -1) - 255/2\n",
    "        entityYs = np.packbits(frameEntities[mask,8:16], -1) - 255/2\n",
    "        newXs, newYs = rotateVec(angleDegrees, entityXs, entityYs) \n",
    "        newXs = np.round(newXs + 255/2).astype(np.uint8)\n",
    "        newYs = np.round(newYs + 255/2).astype(np.uint8)\n",
    "        data[frame_i,mask,0:8] = np.unpackbits(newXs, -1)\n",
    "        data[frame_i,mask,8:16] = np.unpackbits(newYs, -1)\n",
    "\n",
    "        targetXs = np.packbits(frameEntities[mask,16:24], -1) - 255/2\n",
    "        targetYs = np.packbits(frameEntities[mask,24:32], -1) - 255/2\n",
    "        newTargetXs, newTargetYs = rotateVec(angleDegrees, targetXs, targetYs) \n",
    "        newTargetXs = np.round(newTargetXs + 255/2).astype(np.uint8)\n",
    "        newTargetYs = np.round(newTargetYs + 255/2).astype(np.uint8)\n",
    "        data[frame_i,mask,16:24] = np.unpackbits(newTargetXs, -1)\n",
    "        data[frame_i,mask,24:32] = np.unpackbits(newTargetYs, -1)\n",
    "\n",
    "    return data\n",
    "    \n",
    "# Probably only useful for predicting win rate, not predicting the right move.\n",
    "def augmentDataSwap(Xs, Ys):\n",
    "    Xs = Xs.copy()\n",
    "    Ys = Ys.copy()\n",
    "    \n",
    "    Xs[:,:,[FRIENDLY_ENCODING_POSITION,ENEMY_ENCODING_POSITION]] = Xs[:,:,[ENEMY_ENCODING_POSITION,FRIENDLY_ENCODING_POSITION]]\n",
    "    Ys = 1 - Ys\n",
    "    \n",
    "    return Xs, Ys\n",
    "\n",
    "def getFullAugmentedData(Xs, Ys):\n",
    "    allXs = Xs.copy()\n",
    "    allYs = Ys.copy()\n",
    "    \n",
    "    allXs = np.concatenate([allXs, augmentDataMirror(allXs)])\n",
    "    allYs = np.concatenate([allYs, allYs])\n",
    "    \n",
    "    allXs = np.concatenate([allXs, augmentDataRotate(allXs, 180)])\n",
    "    allYs = np.concatenate([allYs, allYs])\n",
    "    \n",
    "    allXs = np.concatenate([allXs, augmentDataRotate(allXs, 15), augmentDataRotate(allXs, -15)])\n",
    "    allYs = np.concatenate([allYs, allYs, allYs])\n",
    "\n",
    "    swappedXs, swappedYs = augmentDataSwap(allXs, allYs)\n",
    "    allXs = np.concatenate([allXs, swappedXs])\n",
    "    allYs = np.concatenate([allYs, swappedYs])\n",
    "    \n",
    "    return allXs, allYs\n",
    "\n",
    "# TESTS:    \n",
    "def testAugmentDataMirrorReversible():\n",
    "    oldData = testEntityEmbeddings\n",
    "    newData = augmentDataMirror(oldData)\n",
    "    doubleReverse = augmentDataMirror(newData)\n",
    "    assert(np.array_equal(oldData, doubleReverse))\n",
    "    assert(not np.array_equal(oldData, newData))\n",
    "    \n",
    "def testAugmentDataRotateReversible_180():\n",
    "    oldData = testEntityEmbeddings\n",
    "    newData = augmentDataRotate(oldData, 180)\n",
    "    doubleReverse = augmentDataRotate(newData, 180)\n",
    "    assert(np.array_equal(oldData, doubleReverse))\n",
    "    assert(not np.array_equal(oldData, newData))\n",
    "    \n",
    "def testAugmentDataRotateReversible_90(): \n",
    "    oldData = testEntityEmbeddings\n",
    "    new1 = augmentDataRotate(oldData, 90)\n",
    "    new2 = augmentDataRotate(new1, 90)\n",
    "    new3 = augmentDataRotate(new2, 90)\n",
    "    new4 = augmentDataRotate(new3, 90)\n",
    "    assert(np.array_equal(oldData, new4))\n",
    "    assert(not np.array_equal(oldData, new3))\n",
    "    assert(not np.array_equal(oldData, new2))\n",
    "    assert(not np.array_equal(oldData, new1))\n",
    "    \n",
    "def testAugmentDataRotateReversible_anyAngle():\n",
    "    oldData = testEntityEmbeddings\n",
    "    \n",
    "    # to avoid rounding issues... same rounding issues will happen when rotated by 90 degrees, so they cancel!\n",
    "    dataDirect = augmentDataRotate(oldData, 12.345)\n",
    "    assert(not np.array_equal(dataDirect, oldData))\n",
    "    \n",
    "    dataIndirect = augmentDataRotate(oldData, 90)\n",
    "    assert(not np.array_equal(dataIndirect, oldData))\n",
    "    dataIndirect = augmentDataRotate(dataIndirect, 12.3452)\n",
    "    assert(not np.array_equal(dataIndirect, oldData))\n",
    "    dataIndirect = augmentDataRotate(dataIndirect, -90)\n",
    "    assert(not np.array_equal(dataIndirect, oldData))\n",
    "\n",
    "    assert(np.array_equal(dataDirect, dataIndirect))\n",
    "\n",
    "def testAugmentDataSwapReversible():\n",
    "    oldXs = np.copy(testX)\n",
    "    oldYs = np.copy(testY)\n",
    "    \n",
    "    newX, newY = augmentDataSwap(oldXs, oldYs)\n",
    "    restoredX, restoredY = augmentDataSwap(newX, newY)\n",
    "    \n",
    "    assert(not np.array_equal(oldXs, newX))\n",
    "    assert(not np.array_equal(oldYs, newY))\n",
    "    \n",
    "    assert(np.array_equal(oldXs, restoredX))\n",
    "    assert(np.array_equal(oldYs, restoredY))\n",
    "        \n",
    "# testAugmentDataMirrorReversible()\n",
    "# testAugmentDataRotateReversible_180()\n",
    "# testAugmentDataRotateReversible_90()\n",
    "# testAugmentDataRotateReversible_anyAngle()\n",
    "# testAugmentDataSwapReversible()\n",
    "# x, y = getFullAugmentedData(testX, testY)\n",
    "    \n",
    "# augmentDataSwap(testEntityEmbeddings, Y)\n",
    "    # TODO: should the data be rotated any more in any direction? maybe a few degrees in either direction etc?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras_transformer.position import TransformerCoordinateEmbedding\n",
    "# from keras_transformer.transformer import TransformerACT, TransformerBlock\n",
    "\n",
    "# inputLayer = Input(shape=testEmbedding.shape)\n",
    "\n",
    "# # add_coordinate_embedding(inputLayer, step=step)\n",
    "\n",
    "# # transformerDepth = 3\n",
    "\n",
    "# # transformer_block = TransformerBlock(\n",
    "# #     name='transformer',\n",
    "# #     num_heads=2, # how many heads?\n",
    "# #     residual_dropout=0.1, # Should I use dropout?\n",
    "# #     attention_dropout=0.1,\n",
    "# #     use_masking=True)\n",
    "# # add_coordinate_embedding = TransformerCoordinateEmbedding(\n",
    "# #     transformerDepth,\n",
    "# #     name='coordinate_embedding')\n",
    "    \n",
    "# # output = inputLayer # shape: (<batch size>, <sequence length>, <input size>)\n",
    "# # for step in range(transformerDepth):\n",
    "# #     output = transformer_block(\n",
    "# #         )\n",
    "\n",
    "# model = Sequential()\n",
    "# model.add(output)\n",
    "# plot_model(model, to_file='models/model_plot.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_SPATIAL_BUCKETS = 32\n",
    "UINT8_TO_BUCKET = (NUM_SPATIAL_BUCKETS - 1)/255\n",
    "\n",
    "# TODO: Implement this as a LAYER instead!!!\n",
    "def scatterEntitiesIntoMapLayer(gameEntityEmbeddings):\n",
    "    frameMapDataShape = (len(gameEntityEmbeddings), NUM_SPATIAL_BUCKETS, NUM_SPATIAL_BUCKETS, EMBEDDED_ENTITY_LENGTH - 16)\n",
    "    gameMapData = np.zeros(frameMapDataShape)\n",
    "    for i, frameEntities in enumerate(gameEntityEmbeddings):\n",
    "        mask = np.sum(frameEntities[...,0:8], -1) >= 0\n",
    "        entityDataOnly = frameEntities[mask]\n",
    "        xIndex = (np.packbits(entityDataOnly[...,0:8], -1) * UINT8_TO_BUCKET).astype(int)\n",
    "        yIndex = (np.packbits(entityDataOnly[...,8:16], -1) * UINT8_TO_BUCKET).astype(int)\n",
    "        indices = np.column_stack((xIndex, yIndex))\n",
    "        for e,[x,y] in enumerate(indices):\n",
    "            gameMapData[i,x,y] += entityDataOnly[e,16:]\n",
    "    \n",
    "    # TODO: could decrease precision probably to speed things up?\n",
    "    return gameMapData.astype(np.int32)\n",
    "                \n",
    "# testSpatialEmbedding = scatterEntitiesIntoMapLayer(testEntityEmbeddings)\n",
    "# print(testSpatialEmbedding.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import TensorShape\n",
    "\n",
    "class ScatterLayer(Layer):\n",
    "    \n",
    "    def __init__(self, numBuckets):\n",
    "        super(ScatterLayer, self).__init__()\n",
    "        self.numBuckets = numBuckets\n",
    "        self.uintToBucket = (numBuckets - 1)/255\n",
    "        self.bitToIntMatrix = tf.constant([128, 64, 32, 16, 8, 4, 2, 1], dtype=tf.float32)\n",
    "    \n",
    "    def build(self, inputShape):\n",
    "        self.outputShape = inputShape[:-2] + (self.numBuckets, self.numBuckets, inputShape[-1]) \n",
    "    \n",
    "    def call(self, input):\n",
    "        xIndices = tf.reduce_sum(input[...,0:8] * self.bitToIntMatrix, -1)\n",
    "        xIndices = tf.math.scalar_mul(self.uintToBucket, xIndices)\n",
    "        xIndices = tf.math.round(xIndices)\n",
    "        presentMask = tf.math.greater_equal(xIndices, 0)\n",
    "        \n",
    "        # HACK: add 0s to (0,0) for a no-op\n",
    "        xIndices = tf.clip_by_value(xIndices, 0, self.numBuckets - 1)\n",
    "        \n",
    "        yIndices = tf.reduce_sum(input[...,8:16] * self.bitToIntMatrix, -1)\n",
    "        yIndices = tf.math.scalar_mul(self.uintToBucket, yIndices)\n",
    "        yIndices = tf.math.round(yIndices)\n",
    "        # HACK: add 0s to (0,0) for a no-op\n",
    "        yIndices = tf.clip_by_value(yIndices, 0, self.numBuckets - 1)\n",
    "        \n",
    "        # missing entities will be less than 0 and masked out\n",
    "        \n",
    "        bitMask = tf.cast(presentMask, tf.float)\n",
    "    \n",
    "#         scatterIndicesX = tf.boolean_mask(xIndices, presentMask, 0)\n",
    "#         scatterIndicesY = tf.boolean_mask(yIndices, presentMask, 0)\n",
    "#         scatterIndices = tf.reshape(scatterIndices, [-1, presentMask.shape[1], *scatterIndices.shape[2:]])\n",
    "        \n",
    "#         updates = tf.boolean_mask(input, presentMask)\n",
    "#         updates = tf.reshape(updates, [-1, presentMask.shape[1], 74])\n",
    "\n",
    "        # boolean_mask not working. Trying slicing instead?\n",
    "#         print(tf.math.count_nonzero(presentMask))\n",
    "\n",
    "#         SCREW IT. JUST MULTIPLY STUFF BY 0????\n",
    "\n",
    "\n",
    "#         \n",
    "        \n",
    "#         scattered = tf.scatter_nd(\n",
    "#             scatterIndices, updates, shape=self.outputShape\n",
    "#         )\n",
    "#         print(input.shape)\n",
    "#         print(presentMask.shape)\n",
    "#         print(scatterIndicesX.shape)\n",
    "#         print(scatterIndicesY.shape)\n",
    "#         print(\"asdf\")\n",
    "#         print(updates.shape)\n",
    "        return tf.where(presentMask, input * presentMask\n",
    "\n",
    "\n",
    "# Spatial Encoder:\n",
    "#     Inputs: map, entity_embeddings\n",
    "#     Outputs:\n",
    "#         embedded_spatial - A 1D tensor of the embedded map\n",
    "#         map_skip - Tensors of the outputs of intermediate computations\n",
    "def spatial_encoder_model(entityEmbeddings):\n",
    "    inputShape = (MAX_NUM_ENTITIES, EMBEDDED_ENTITY_LENGTH)\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(InputLayer(input_shape=inputShape))\n",
    "    model.add(ScatterLayer(numBuckets=NUM_SPATIAL_BUCKETS))\n",
    "#     model.add(Conv2D(32,1,activation='relu'))\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = spatial_encoder_model(testEmbedding)\n",
    "# model.summary()\n",
    "model.predict(X[0:2],  steps=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras_transformer.position import TransformerCoordinateEmbedding\n",
    "from keras_transformer.transformer import TransformerACT, TransformerBlock\n",
    "\n",
    "def universal_transformer_alphaspark_model(\n",
    "        inputShape: int, \n",
    "        transformer_depth: int,\n",
    "        num_heads: int, \n",
    "        transformer_dropout: float = 0.1,\n",
    "        confidence_penalty_weight: float = 0.1):\n",
    "    \n",
    "    # TODO: pass this in maybe?\n",
    "    inputLayer = Input(shape=inputShape, name='inputLayerrrr') # TODO: can probably change dtype to binary or something?\n",
    "\n",
    "    coordinate_embedding_layer = TransformerCoordinateEmbedding(\n",
    "        transformer_depth,\n",
    "        name='coordinate_embedding')\n",
    "#     transformer_act_layer = TransformerACT(name='adaptive_computation_time')\n",
    "    transformer_block = TransformerBlock(\n",
    "        name='transformerASDF', \n",
    "        num_heads=num_heads,\n",
    "        residual_dropout=transformer_dropout, #  Are these dropouts reasonable???\n",
    "        attention_dropout=transformer_dropout,\n",
    "        use_masking=True, \n",
    "        vanilla_wiring=False)\n",
    "    output_softmax_layer = Softmax(name='predictionsIGuess')\n",
    "\n",
    "#     next_step_input, embedding_matrix = embedding_layer(word_ids)\n",
    "    next_step_input = inputLayer\n",
    "#     act_output = next_step_input\n",
    "\n",
    "    for i in range(transformer_depth):\n",
    "        next_step_input = coordinate_embedding_layer(next_step_input, step=i)\n",
    "        next_step_input = transformer_block(next_step_input)\n",
    "#         next_step_input, act_output = transformer_act_layer(next_step_input)\n",
    "\n",
    "#     transformer_act_layer.finalize()\n",
    "#     next_step_input = act_output\n",
    "\n",
    "    # TODO: this softmax may or may not work.\n",
    "    predictions = output_softmax_layer(next_step_input)\n",
    "    model = Model(inputs=[inputLayer], outputs=[predictions])\n",
    "    # Penalty for confidence of the output distribution, as described in\n",
    "    # \"Regularizing Neural Networks by Penalizing Confident\n",
    "    # Output Distributions\" (https://arxiv.org/abs/1701.06548)\n",
    "#     confidence_penalty = K.mean(\n",
    "#         confidence_penalty_weight *\n",
    "#         K.sum(word_predictions * K.log(word_predictions), axis=-1))\n",
    "#     model.add_loss(confidence_penalty)\n",
    "    return model\n",
    "\n",
    "inputShape = (MAX_NUM_ENTITIES, EMBEDDED_ENTITY_LENGTH)\n",
    "model = universal_transformer_alphaspark_model(inputShape, 1, 2)\n",
    "# print(model)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.layers\n",
    "import keras.regularizers\n",
    "\n",
    "\n",
    "parameters = {\n",
    "    \"kernel_initializer\": \"he_normal\"\n",
    "}\n",
    "\n",
    "BATCH_NORM_MOMENTUM = 0.99\n",
    "BATCH_NORM_EPSILON = 0.001\n",
    "L2_WEIGHT_DECAY = 0.001\n",
    "\n",
    "def createResNetBlock(\n",
    "    numFilters,\n",
    "    stage=0,\n",
    "    block=0,\n",
    "    kernel_size=3\n",
    "):\n",
    "    def f(prev):\n",
    "        y = prev\n",
    "        # TODO: is axis=-1 default correct for batchnormalization?\n",
    "        y = keras.layers.BatchNormalization(\n",
    "                                        momentum=BATCH_NORM_MOMENTUM,\n",
    "                                        epsilon=BATCH_NORM_EPSILON)(y)\n",
    "        y = keras.layers.Activation(\"relu\")(y)\n",
    "        y = keras.layers.Conv2D(numFilters, kernel_size, padding=\"same\", use_bias=False, \n",
    "                                kernel_regularizer=regularizers.l2(L2_WEIGHT_DECAY), **parameters)(y)\n",
    "\n",
    "        y = keras.layers.BatchNormalization(\n",
    "                                        momentum=BATCH_NORM_MOMENTUM,\n",
    "                                        epsilon=BATCH_NORM_EPSILON)(y)\n",
    "        y = keras.layers.Activation(\"relu\")(y)\n",
    "        y = keras.layers.Conv2D(numFilters, kernel_size, padding=\"same\", use_bias=False, \n",
    "                                kernel_regularizer=regularizers.l2(L2_WEIGHT_DECAY), **parameters)(y)\n",
    "\n",
    "        y = keras.layers.Add()([y, prev])\n",
    "\n",
    "        return y\n",
    "    return f\n",
    "\n",
    "def createSpatialEncoder(prev):\n",
    "    # initial kernel size 1 conv\n",
    "    prev = Conv2D(64, 1, padding='same', kernel_regularizer=regularizers.l2(L2_WEIGHT_DECAY))(prev)\n",
    "    prev = BatchNormalization()(prev)\n",
    "    prev = Activation(\"relu\")(prev)\n",
    "\n",
    "    # downsampling\n",
    "    prev = Conv2D(32, 4, strides=2, padding='same', kernel_regularizer=regularizers.l2(L2_WEIGHT_DECAY))(prev)\n",
    "    prev = BatchNormalization()(prev)\n",
    "    prev = Activation(\"relu\")(prev)\n",
    "\n",
    "    # TODO: ResBlocks\n",
    "\n",
    "    prev = createResNetBlock(32)(prev)\n",
    "    prev = createResNetBlock(32)(prev)\n",
    "    prev = createResNetBlock(32)(prev)\n",
    "    prev = createResNetBlock(32)(prev)\n",
    "    \n",
    "    prev = MaxPooling2D()(prev)\n",
    "    prev = GlobalAveragePooling2D()(prev)\n",
    "    prev = Dropout(0.1)(prev)\n",
    "    prev = Dense(8, \n",
    "                 activation=\"relu\",\n",
    "                 kernel_regularizer=regularizers.l2(L2_WEIGHT_DECAY),\n",
    "                 bias_regularizer=regularizers.l2(L2_WEIGHT_DECAY)\n",
    "                )(prev)\n",
    "    prev = Dropout(0.1)(prev)\n",
    "    prev = Dense(1, \n",
    "                 activation=\"sigmoid\",\n",
    "                 kernel_regularizer=regularizers.l2(L2_WEIGHT_DECAY),\n",
    "                 bias_regularizer=regularizers.l2(L2_WEIGHT_DECAY)\n",
    "                )(prev)\n",
    "    return prev\n",
    "\n",
    "\n",
    "inputShape = (NUM_SPATIAL_BUCKETS, NUM_SPATIAL_BUCKETS, EMBEDDED_ENTITY_LENGTH - 16)\n",
    "inputLayer = Input(shape=inputShape)\n",
    "outputLayer = createSpatialEncoder(inputLayer)\n",
    "\n",
    "model = Model(inputs=[inputLayer], outputs=[outputLayer])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# compiling the model\n",
    "model.compile(\n",
    " optimizer = \"adam\", #just gonna hope this is correct\n",
    " loss = \"binary_crossentropy\", # IS THIS CORRECT ANYMORE??? I think this is appropriate because the model classifies win/loss with probability in range (0-1)\n",
    " metrics = [\"accuracy\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BVZKzi6-f32e"
   },
   "outputs": [],
   "source": [
    "plot_model(model, to_file='models/model_plot.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iCHCZ61a5x9x"
   },
   "outputs": [],
   "source": [
    "model.save_weights('models/model_resnet_2_pretrain.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RclehELr5yBo"
   },
   "outputs": [],
   "source": [
    "model.load_weights('models/model_resnet_2_pretrain.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "e7kPLL3WfRcq"
   },
   "outputs": [],
   "source": [
    "# def trainFromFile(model, filepath, numGamesPerEpisode=10):\n",
    "#     games = []\n",
    "#     with open(filepath) as f:\n",
    "#         for line in f:\n",
    "#             games.append(json.loads(line))\n",
    "#             if len(games) >= numGamesPerEpisode:\n",
    "#                 trainFromGames(model, games)\n",
    "#                 games = []\n",
    "#         if len(games) > 0:\n",
    "#             trainFromGames(model, games)\n",
    "#             games = []\n",
    "       \n",
    "    \n",
    "# def trainFromGames(model, games):\n",
    "#     print(\"Training on {} games\".format(len(games)))\n",
    "    \n",
    "#     X = []\n",
    "#     Y = []\n",
    "     \n",
    "#     for game in games:\n",
    "#         X.extend(getGameEntityEmbeddings(game))\n",
    "#         Y.extend(getTrainYForGame(game))\n",
    "\n",
    "#     # TEMPORARY: Train on every 10th frame to avoid overfitting?\n",
    "#     X = np.array(X)\n",
    "#     Y = np.array(Y)\n",
    "# #     X = np.array(X)[::10]\n",
    "# #     Y = np.array(Y)[::10]    \n",
    "    \n",
    "#     model.fit(X, Y, validation_split=0.1, epochs=1, batch_size=32, shuffle=True)   \n",
    "\n",
    "    \n",
    "# trainFromFile(model, 'D:\\GalconZero\\Waffle\\\\train_test.jsonl', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use only 1/SKIP_FRAME_RATIO frames to avoid overfitting\n",
    "SKIP_FRAME_RATIO = 10\n",
    "\n",
    "history = None\n",
    "\n",
    "def trainFromFile(model, filepath, numGamesPerEpisode=10):\n",
    "    games = []\n",
    "    with open(filepath) as f:\n",
    "        for line in f:\n",
    "            games.append(json.loads(line))\n",
    "            if len(games) >= numGamesPerEpisode:\n",
    "                trainFromGames(model, games)\n",
    "                return\n",
    "                games = []\n",
    "        if len(games) > 0:\n",
    "            trainFromGames(model, games)\n",
    "            games = []\n",
    "            \n",
    "def trainFromGames(model, games):\n",
    "    print(\"Training on {} games\".format(len(games)))\n",
    "    \n",
    "    X = []\n",
    "    Y = []\n",
    "    \n",
    "    for game in games:\n",
    "        game['frames'] = game['frames'][::SKIP_FRAME_RATIO]\n",
    "        \n",
    "        entityEmbeddings = getGameEntityEmbeddings(game)\n",
    "        gameY = getTrainYForGame(game)\n",
    "        \n",
    "        augmentX, augmentY = getFullAugmentedData(entityEmbeddings, gameY)\n",
    "\n",
    "        mapEmbedding = scatterEntitiesIntoMapLayer(augmentX)\n",
    "        \n",
    "        X.extend(mapEmbedding)\n",
    "        Y.extend(augmentY)\n",
    "\n",
    "    X = np.array(X)\n",
    "    Y = np.array(Y)  \n",
    "    \n",
    "    global history\n",
    "    newHistory = model.fit(X, Y, validation_split=0.2, epochs=1, batch_size=32, shuffle=True)   \n",
    "#     newHistory = model.evaluate(X, Y, batch_size=32)\n",
    "#     print(model.metrics_names)\n",
    "#     print(newHistory)\n",
    "    if history == None:\n",
    "        history = newHistory\n",
    "        print(\"none\")\n",
    "    for k in newHistory.history.keys():\n",
    "        history.history[k].extend(newHistory.history[k])\n",
    "\n",
    "    \n",
    "# trainFromFile(model, 'D:\\GalconZero\\Waffle\\\\train_test_SMALL.jsonl', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import h5py\n",
    "\n",
    "DATA_DIR = \"D:/GalconZero/Waffle\"\n",
    "TRAINING_DATA_DB_FILENAME = \"trainingData_db.hdf5\"\n",
    "SKIP_FRAME_RATIO = 10\n",
    "NUM_GAMES_PER_CHUNK = 100\n",
    "TEST_SPLIT_RATIO = 0.15\n",
    "VALIDATION_SPLIT_RATIO = 0.15\n",
    "\n",
    "X_TRAIN = \"xTrain\"\n",
    "Y_TRAIN = \"yTrain\"\n",
    "X_TEST = \"xTest\"\n",
    "Y_TEST = \"yTest\"\n",
    "\n",
    "def getTrainYForGame(game):\n",
    "    playerWon = int(game[\"winnerN\"] == game[\"playerN\"])\n",
    "    numFrames = len(game['frames'])\n",
    "    return np.repeat(playerWon, numFrames)\n",
    "\n",
    "def saveTrainingDataFromFile(directory, replayFilename):\n",
    "    games = []\n",
    "    with open(f\"{directory}/{replayFilename}\") as f:\n",
    "        for line in f:\n",
    "            games.append(json.loads(line))\n",
    "            if len(games) >= NUM_GAMES_PER_CHUNK:\n",
    "                saveTrainingData(games, directory)\n",
    "                games = []\n",
    "        if len(games) > 0:\n",
    "            saveTrainingData(games, directory)\n",
    "            games = []\n",
    "\n",
    "def saveTrainingData(games, directory):\n",
    "    print(\"Saving data for {} games\".format(len(games)))\n",
    "    \n",
    "    X = []\n",
    "    Y = []\n",
    "    \n",
    "    for game in games:\n",
    "        # use only 1/SKIP_FRAME_RATIO frames to avoid overfitting\n",
    "        game['frames'] = game['frames'][::SKIP_FRAME_RATIO]\n",
    "        \n",
    "        X.append(getGameEntityEmbeddings(game))\n",
    "        Y.append(getTrainYForGame(game))\n",
    "\n",
    "    X = np.concatenate(X)\n",
    "    Y = np.concatenate(Y)\n",
    "    assert(X.shape[0]== Y.shape[0])\n",
    "    \n",
    "    numTestSamples = round(X.shape[0] * TEST_SPLIT_RATIO)\n",
    "    assert(numTestSamples > 0)\n",
    "    \n",
    "    xTrain = X[:-numTestSamples]\n",
    "    xTest = X[-numTestSamples:]\n",
    "    assert(xTrain.shape[0] + xTest.shape[0] == X.shape[0])\n",
    "    \n",
    "    yTrain = Y[:-numTestSamples]\n",
    "    yTest = Y[-numTestSamples:]\n",
    "    assert(yTrain.shape[0] + yTest.shape[0] == Y.shape[0])\n",
    "\n",
    "    saveToHdf5(directory, xTrain, yTrain, xTest, yTest)\n",
    "    \n",
    "def saveToHdf5(directory, xTrain, yTrain, xTest, yTest):\n",
    "    dbFilepath = f\"{directory}/{TRAINING_DATA_DB_FILENAME}\"\n",
    "    with h5py.File(dbFilepath, 'a') as hf:\n",
    "        appendData(hf, X_TRAIN, xTrain)\n",
    "        appendData(hf, Y_TRAIN, yTrain)\n",
    "        appendData(hf, X_TEST, xTest)\n",
    "        appendData(hf, Y_TEST, yTest)\n",
    "    \n",
    "def appendData(hf, datasetName, data):\n",
    "    if datasetName in hf:\n",
    "        hf[datasetName].resize((hf[datasetName].shape[0] + data.shape[0]), axis = 0)\n",
    "        hf[datasetName][-data.shape[0]:] = data\n",
    "    else:\n",
    "        addDataset(hf, datasetName, data)\n",
    "        \n",
    "# TODO: shuffle dataset\n",
    "def addDataset(hf, datasetName, data):\n",
    "    # TODO: can remove chunks=True? What is the default chunk size? is maxShape correct?\n",
    "    maxShape = (None, *data.shape[1:])\n",
    "    hf.create_dataset(datasetName, data=data, compression=\"gzip\", chunks=True, maxshape=maxShape)\n",
    "    \n",
    "def fetchDataset(directory, hdf5Filename, xDatasetName, yDatasetName):\n",
    "    dbFilepath = f\"{directory}/{hdf5Filename}\"\n",
    "    with h5py.File(dbFilepath, 'r') as hf:\n",
    "        # TODO: return a generator instead because this will exceed memory size soon, or maybe not necessary because I can index and it'll fetch lazily???\n",
    "        # Or instead do a HDF5Matrix which might take care of this slicing stuff. Figure this out with print(len( etc))\n",
    "        # \"Not a dataset\" error means we have closed the file.\n",
    "#         xDataset = HDF5Matrix(dbFilepath, xDatasetName, end=100)\n",
    "#         yDataset = HDF5Matrix(dbFilepath, yDatasetName, end=100)\n",
    "        return hf[xDatasetName][0:2400], hf[yDatasetName][0:2400]\n",
    "#     return xDataset, yDataset\n",
    "     \n",
    "def testFromHdf5File(model, directory, replayFilename):\n",
    "    xTest, yTest = fetchDataset(directory, hdf5Filename, X_TEST, Y_TEST)\n",
    "    \n",
    "    augmentX, augmentY = getFullAugmentedData(xTest, yTest)\n",
    "    \n",
    "    # TODO: streamline this with a scatter layer instead of manual code.\n",
    "    mapEmbedding = scatterEntitiesIntoMapLayer(augmentX)\n",
    "    \n",
    "    X = mapEmbedding\n",
    "    Y = augmentY\n",
    "    \n",
    "    results = model.evaluate(X, Y, batch_size=32)\n",
    "    print(results)\n",
    "    \n",
    "def trainFromHdf5File(model, directory, hdf5Filename):\n",
    "    xTrain, yTrain = fetchDataset(directory, hdf5Filename, X_TRAIN, Y_TRAIN)\n",
    "    \n",
    "#     xTrain, yTrain = getFullAugmentedData(xTrain, yTrain)\n",
    "    \n",
    "    # TODO: streamline this with a scatter layer instead of manual code.\n",
    "    xTrain = scatterEntitiesIntoMapLayer(xTrain)\n",
    "    \n",
    "    X = xTrain\n",
    "    Y = yTrain\n",
    "    \n",
    "    history = model.fit(X, Y, validation_split=VALIDATION_SPLIT_RATIO, epochs=1, batch_size=32, shuffle=True)   \n",
    "    \n",
    "# # history = None\n",
    "# a,b = fetchDataFromHdf5(DATA_DIR, \"trainingData_db_small.hdf5\", X_TRAIN, Y_TRAIN)\n",
    "# print(a.shape, b.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "start = time.time()\n",
    "trainFromFile(model, 'D:\\GalconZero\\Waffle\\\\train1.jsonl', 30)\n",
    "print(\"TIME ELAPSED\", time.time() - start)\n",
    "\n",
    "start = time.time()\n",
    "trainFromHdf5File(model, DATA_DIR, \"trainingData_db_small.hdf5\")\n",
    "print(\"TIME ELAPSED\", time.time() - start)\n",
    "\n",
    "start = time.time()\n",
    "trainFromHdf5File(model, DATA_DIR, \"trainingData_db_small.hdf5\")\n",
    "print(\"TIME ELAPSED\", time.time() - start)\n",
    "\n",
    "start = time.time()\n",
    "trainFromFile(model, 'D:\\GalconZero\\Waffle\\\\train1.jsonl', 30)\n",
    "print(\"TIME ELAPSED\", time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saveTrainingDataFromFile(\"D:/GalconZero/Waffle\", \"train1.jsonl\")\n",
    "# saveTrainingDataFromFile(\"D:/GalconZero/Waffle\", \"train2.jsonl\")\n",
    "# saveTrainingDataFromFile(\"D:/GalconZero/Waffle\", \"train3.jsonl\")\n",
    "# saveTrainingDataFromFile(\"D:/GalconZero/Waffle\", \"train4.jsonl\")\n",
    "# saveTrainingDataFromFile(\"D:/GalconZero/Waffle\", \"train5.jsonl\")\n",
    "# saveTrainingDataFromFile(\"D:/GalconZero/Waffle\", \"train6.jsonl\")\n",
    "# saveTrainingDataFromFile(\"D:/GalconZero/Waffle\", \"train7.jsonl\")\n",
    "# saveTrainingDataFromFile(\"D:/GalconZero/Waffle\", \"train8.jsonl\")\n",
    "# saveTrainingDataFromFile(\"D:/GalconZero/Waffle\", \"train9.jsonl\")\n",
    "# saveTrainingDataFromFile(\"D:/GalconZero/Waffle\", \"train10.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST TRAINING\n",
    "# trainFromFile(model, 'D:\\GalconZero\\Waffle\\\\train_test_SMALL.jsonl', 1)\n",
    "trainFromFile(model, 'D:\\GalconZero\\Waffle\\\\testData.jsonl', 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def countLines(directory, filename):\n",
    "    lines = 0\n",
    "    with open(f\"{directory}/{filename}\") as f:\n",
    "        for line in f:\n",
    "            game = json.loads(line)\n",
    "            lines += len(game[\"frames\"])\n",
    "    print(\"file \", lines)\n",
    "    return lines\n",
    "\n",
    "x = 0\n",
    "# x += countLines(\"D:/GalconZero/Waffle\", \"train1.jsonl\")\n",
    "# x += countLines(\"D:/GalconZero/Waffle\", \"train2.jsonl\")\n",
    "# x += countLines(\"D:/GalconZero/Waffle\", \"train3.jsonl\")\n",
    "# x += countLines(\"D:/GalconZero/Waffle\", \"train4.jsonl\")\n",
    "# x += countLines(\"D:/GalconZero/Waffle\", \"train5.jsonl\")\n",
    "# x += countLines(\"D:/GalconZero/Waffle\", \"train6.jsonl\")\n",
    "# x += countLines(\"D:/GalconZero/Waffle\", \"train7.jsonl\")\n",
    "# x += countLines(\"D:/GalconZero/Waffle\", \"train8.jsonl\")\n",
    "# x += countLines(\"D:/GalconZero/Waffle\", \"train9.jsonl\")\n",
    "print(x)\n",
    "# should equal 1677147 / 0.085 = 19.7M\n",
    "# 17,331,412\n",
    "\n",
    "# 130110 for train1?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "    \n",
    "print(history.history.keys())  \n",
    "   \n",
    "plt.figure(1)  \n",
    "   \n",
    "# summarize history for accuracy  \n",
    "   \n",
    "plt.subplot(211)  \n",
    "plt.plot(history.history['acc'])  \n",
    "# plt.plot(history.history['val_acc'])    \n",
    "plt.title('model accuracy')  \n",
    "plt.ylabel('accuracy')  \n",
    "plt.xlabel('epoch')  \n",
    "plt.legend(['train', 'test'], loc='upper left')  \n",
    "   \n",
    "# summarize history for loss  \n",
    "   \n",
    "plt.subplot(212)  \n",
    "plt.plot(history.history['loss'])  \n",
    "# plt.plot(history.history['val_loss'])  \n",
    "plt.title('model loss')  \n",
    "plt.ylabel('loss')  \n",
    "plt.xlabel('epoch')  \n",
    "plt.legend(['train', 'test'], loc='upper left')  \n",
    "plt.show()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainFromFile(model, 'D:\\GalconZero\\Waffle\\\\train6.jsonl', 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7PXxu5eD4f8t"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "9v4LDHW93rDD",
    "outputId": "b566df86-015b-425e-b667-3144e1093fcf"
   },
   "outputs": [],
   "source": [
    "# history = model.fit(X, Y, validation_split=0.33, epochs=1, batch_size=8, shuffle=True)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QBIwpCBUbd2n"
   },
   "outputs": [],
   "source": [
    "# Entity Encoder:\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "AlphaSpark.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
