{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Anaconda3\\envs\\GalconZero\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 22, 15)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 22, 10)       160         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)       (None, 22, 10)       0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 22, 8)        88          leaky_re_lu_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)       (None, 22, 8)        0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 176)          0           leaky_re_lu_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 100)          17700       flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 20)           3540        flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)       (None, 100)          0           dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)       (None, 20)           0           dense_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 100)          10100       leaky_re_lu_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 15)           315         leaky_re_lu_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)       (None, 100)          0           dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_7 (LeakyReLU)       (None, 15)           0           dense_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 100)          10100       leaky_re_lu_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 7)            112         leaky_re_lu_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)       (None, 100)          0           dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_8 (LeakyReLU)       (None, 7)            0           dense_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "policy (Dense)                  (None, 925)          93425       leaky_re_lu_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "value (Dense)                   (None, 1)            8           leaky_re_lu_8[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 135,548\n",
      "Trainable params: 135,548\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Activation, Flatten, LeakyReLU, Dropout\n",
    "from keras.utils import plot_model\n",
    "from nnSetup import NUM_PLANETS, NUM_FEATURES, NUM_OUTPUTS\n",
    "\n",
    "RELU_ALPHA = 0.05\n",
    "DROPOUT = 0.1\n",
    "\n",
    "# INPUTS\n",
    "inputs = Input(shape=(NUM_PLANETS, NUM_FEATURES))\n",
    "\n",
    "# BODY\n",
    "# TODO: insert residual GCNNs layers here\n",
    "# body = Dropout(DROPOUT)(inputs)\n",
    "# body = Dense(10)(body)\n",
    "body = Dense(10)(inputs)\n",
    "body = LeakyReLU(alpha=RELU_ALPHA)(body)\n",
    "\n",
    "# body = Dropout(DROPOUT)(body)\n",
    "body = Dense(8)(body)\n",
    "body = LeakyReLU(alpha=RELU_ALPHA)(body)\n",
    "\n",
    "body = Flatten()(body)\n",
    "\n",
    "# POLICY HEAD\n",
    "# policyHead = Dropout(DROPOUT)(body)\n",
    "# policyHead = Dense(100)(policyHead)\n",
    "policyHead = Dense(100)(body)\n",
    "policyHead = LeakyReLU(alpha=RELU_ALPHA)(policyHead)\n",
    "\n",
    "# policyHead = Dropout(DROPOUT)(policyHead)\n",
    "policyHead = Dense(100)(policyHead)\n",
    "policyHead = LeakyReLU(alpha=RELU_ALPHA)(policyHead)\n",
    "\n",
    "# policyHead = Dropout(DROPOUT)(policyHead)\n",
    "policyHead = Dense(100)(policyHead)\n",
    "policyHead = LeakyReLU(alpha=RELU_ALPHA)(policyHead)\n",
    "\n",
    "policyHead = Dense(NUM_OUTPUTS, activation='softmax', name='policy')(policyHead)\n",
    "\n",
    "# VALUE HEAD\n",
    "# valueHead = Dropout(DROPOUT)(body)\n",
    "# valueHead = Dense(20)(valueHead)\n",
    "valueHead = Dense(20)(body)\n",
    "valueHead = LeakyReLU(alpha=RELU_ALPHA)(valueHead)\n",
    "\n",
    "# valueHead = Dropout(DROPOUT)(valueHead)\n",
    "valueHead = Dense(15)(valueHead)\n",
    "valueHead = LeakyReLU(alpha=RELU_ALPHA)(valueHead)\n",
    "\n",
    "# valueHead = Dropout(DROPOUT)(valueHead)\n",
    "valueHead = Dense(7)(valueHead)\n",
    "valueHead = LeakyReLU(alpha=RELU_ALPHA)(valueHead)\n",
    "\n",
    "valueHead = Dense(1, activation='sigmoid', name='value')(valueHead)\n",
    "\n",
    "# TODO: multiple percentages\n",
    "model = Model(inputs=inputs, outputs=[policyHead, valueHead], name='GZ_DenseNetwork')\n",
    "\n",
    "\n",
    "print(model.summary())\n",
    "plot_model(model, to_file='model_graph.png')\n",
    "\n",
    "# TODO: are these losses right??\n",
    "# TODO: measure losses to get better loss weights?\n",
    "# TODO: are these metrics right?\n",
    "model.compile(loss=['categorical_crossentropy', 'binary_crossentropy'], loss_weights=[1, 10], optimizer='adam', metrics = [\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trainingGame import loadTrainingGame\n",
    "from trainingHelper import TrainingHelper\n",
    "from mapHelper import mapHelper\n",
    "\n",
    "game = loadTrainingGame(\"testGame.pickle\")\n",
    "exampleState = game.states[0]\n",
    "mapHelper.setItems(exampleState)\n",
    "helper = TrainingHelper(game) \n",
    "\n",
    "trainX = helper.getTrainX()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[0.00109572, 0.00115127, 0.00108457, 0.00106449, 0.00106956,\n",
       "         0.00111303, 0.00104575, 0.00105756, 0.00103689, 0.00106757,\n",
       "         0.0010857 , 0.0010772 , 0.0010796 , 0.00111218, 0.00109678,\n",
       "         0.00107504, 0.00110143, 0.00110853, 0.00107454, 0.00108431,\n",
       "         0.0010993 , 0.00109439, 0.00105458, 0.00103354, 0.00111998,\n",
       "         0.0010334 , 0.00109089, 0.00107926, 0.00107107, 0.00104932,\n",
       "         0.00115712, 0.0010962 , 0.0010482 , 0.00108113, 0.00104424,\n",
       "         0.00109219, 0.00110627, 0.00115162, 0.00108645, 0.00108825,\n",
       "         0.00105061, 0.00107302, 0.00105871, 0.00108201, 0.00114438,\n",
       "         0.00114682, 0.00102315, 0.00108227, 0.00106743, 0.00103917,\n",
       "         0.00118035, 0.00108853, 0.0010788 , 0.00107914, 0.00112927,\n",
       "         0.00109598, 0.00107007, 0.00106373, 0.00111498, 0.00102602,\n",
       "         0.00106755, 0.00100197, 0.00105872, 0.00111386, 0.00102517,\n",
       "         0.00114539, 0.00112751, 0.00109864, 0.0011337 , 0.0010805 ,\n",
       "         0.00109298, 0.00104221, 0.00111871, 0.0010657 , 0.00111759,\n",
       "         0.00108396, 0.00109309, 0.00106382, 0.0010274 , 0.0010318 ,\n",
       "         0.00106841, 0.00108702, 0.00111684, 0.00109422, 0.00112889,\n",
       "         0.00108567, 0.00109612, 0.0010613 , 0.00108543, 0.00114146,\n",
       "         0.00109397, 0.00111803, 0.00105205, 0.00106647, 0.00107264,\n",
       "         0.00107698, 0.00113336, 0.00109508, 0.00101521, 0.00111979,\n",
       "         0.00101207, 0.00113586, 0.00109264, 0.0010267 , 0.00103606,\n",
       "         0.00109735, 0.00108692, 0.00109694, 0.00109691, 0.00115536,\n",
       "         0.0010934 , 0.0010865 , 0.00105755, 0.00105004, 0.00106659,\n",
       "         0.00109956, 0.00107096, 0.0010795 , 0.00112909, 0.00104925,\n",
       "         0.00106553, 0.00109067, 0.00109674, 0.00110993, 0.00111209,\n",
       "         0.00111404, 0.00109966, 0.00108219, 0.00102489, 0.00103386,\n",
       "         0.00108114, 0.0010347 , 0.00104426, 0.00108154, 0.00103773,\n",
       "         0.00111417, 0.00108922, 0.00110003, 0.00103987, 0.00111828,\n",
       "         0.00110354, 0.00098711, 0.00104481, 0.00106977, 0.00112664,\n",
       "         0.00102026, 0.00105217, 0.00106227, 0.00101976, 0.00112946,\n",
       "         0.00107581, 0.00106973, 0.00112558, 0.00103637, 0.00107386,\n",
       "         0.00117375, 0.00099401, 0.0010612 , 0.00107501, 0.00111388,\n",
       "         0.00106094, 0.00104733, 0.00109311, 0.00111982, 0.00108635,\n",
       "         0.00101919, 0.00112903, 0.00108699, 0.00105265, 0.00110278,\n",
       "         0.00113551, 0.0010934 , 0.00107793, 0.00115445, 0.00115206,\n",
       "         0.00113072, 0.00104845, 0.001031  , 0.00112172, 0.00102983,\n",
       "         0.00102154, 0.00112255, 0.00108143, 0.00109096, 0.00116182,\n",
       "         0.00110058, 0.00110821, 0.00107244, 0.00111109, 0.00104514,\n",
       "         0.00103808, 0.00112129, 0.00105911, 0.00102488, 0.00107077,\n",
       "         0.00101923, 0.00108999, 0.00105415, 0.00108839, 0.00112631,\n",
       "         0.00109957, 0.00106626, 0.00112453, 0.00107617, 0.00109273,\n",
       "         0.00104092, 0.00107113, 0.00102967, 0.00102891, 0.0010927 ,\n",
       "         0.00110738, 0.00107431, 0.00112635, 0.00107003, 0.00110326,\n",
       "         0.00105971, 0.00105995, 0.00106313, 0.00113739, 0.00100838,\n",
       "         0.00113669, 0.00112239, 0.00109297, 0.0010801 , 0.00107182,\n",
       "         0.00109098, 0.00104927, 0.00108974, 0.0011377 , 0.00110459,\n",
       "         0.00112443, 0.00107618, 0.0010392 , 0.00108133, 0.00103532,\n",
       "         0.00104968, 0.00106925, 0.00109427, 0.00111663, 0.00102959,\n",
       "         0.00106926, 0.0010776 , 0.00105882, 0.00107375, 0.0011619 ,\n",
       "         0.00109535, 0.00110778, 0.00110902, 0.00107463, 0.00109276,\n",
       "         0.00109881, 0.00109731, 0.00103115, 0.00110661, 0.00104584,\n",
       "         0.00107893, 0.00114161, 0.00109731, 0.001055  , 0.00112969,\n",
       "         0.00108012, 0.00111309, 0.00109533, 0.00105471, 0.00113227,\n",
       "         0.00110943, 0.00109216, 0.00109713, 0.00107672, 0.00112509,\n",
       "         0.00105677, 0.00108809, 0.00112221, 0.00109697, 0.00109481,\n",
       "         0.00104843, 0.00108649, 0.00114743, 0.00107972, 0.00109855,\n",
       "         0.00109704, 0.00105194, 0.00107824, 0.00107865, 0.0010746 ,\n",
       "         0.00104065, 0.00101994, 0.00108438, 0.00113618, 0.00105794,\n",
       "         0.0010688 , 0.00101638, 0.00109995, 0.00106858, 0.00107489,\n",
       "         0.001095  , 0.00107884, 0.00105564, 0.00112444, 0.00109477,\n",
       "         0.00102433, 0.00107998, 0.00106064, 0.00114909, 0.00108485,\n",
       "         0.00110209, 0.00103507, 0.0010942 , 0.00111158, 0.00106171,\n",
       "         0.00105202, 0.00108542, 0.00105418, 0.00113177, 0.00111726,\n",
       "         0.00107083, 0.00106746, 0.00108913, 0.00114468, 0.00105297,\n",
       "         0.00106937, 0.0010877 , 0.00110901, 0.00105594, 0.00108038,\n",
       "         0.00108152, 0.001136  , 0.00109818, 0.00105265, 0.00107549,\n",
       "         0.00110116, 0.00108035, 0.00103388, 0.00108482, 0.00108461,\n",
       "         0.00104853, 0.00104943, 0.00110908, 0.00110605, 0.00103737,\n",
       "         0.00110564, 0.00112148, 0.00112703, 0.00110101, 0.00114158,\n",
       "         0.00111201, 0.00112758, 0.00103275, 0.00107448, 0.00108225,\n",
       "         0.00105088, 0.00108979, 0.00109075, 0.0011579 , 0.00110098,\n",
       "         0.00109478, 0.00102045, 0.00111314, 0.0010499 , 0.00109549,\n",
       "         0.0010741 , 0.00108038, 0.00110116, 0.00108157, 0.00111893,\n",
       "         0.00108843, 0.00105923, 0.00108495, 0.00104026, 0.00106274,\n",
       "         0.00117246, 0.00108005, 0.0011587 , 0.00110231, 0.00106103,\n",
       "         0.00105479, 0.00111767, 0.0010691 , 0.00103956, 0.00106853,\n",
       "         0.00105824, 0.00105689, 0.00108811, 0.00103031, 0.00112043,\n",
       "         0.00100906, 0.00110273, 0.00111101, 0.0011057 , 0.00111217,\n",
       "         0.00109334, 0.00110279, 0.00112576, 0.00107979, 0.00105985,\n",
       "         0.00100276, 0.00105585, 0.00105798, 0.00116264, 0.00106986,\n",
       "         0.00108656, 0.00108607, 0.00106278, 0.00107993, 0.00104311,\n",
       "         0.00106048, 0.00103497, 0.00105755, 0.00108756, 0.00104648,\n",
       "         0.00108951, 0.00111196, 0.00104973, 0.00111425, 0.00102697,\n",
       "         0.00106856, 0.00109002, 0.00114196, 0.00113104, 0.00108399,\n",
       "         0.00109631, 0.00110041, 0.00104135, 0.001027  , 0.0010129 ,\n",
       "         0.00110507, 0.00110886, 0.00108267, 0.00107414, 0.00110115,\n",
       "         0.00107119, 0.00103805, 0.0010729 , 0.00104611, 0.00108568,\n",
       "         0.00104781, 0.0010938 , 0.00108452, 0.00110918, 0.00108337,\n",
       "         0.00108418, 0.00114417, 0.00107538, 0.00108699, 0.00109477,\n",
       "         0.00111324, 0.00108687, 0.00101088, 0.00106226, 0.00106484,\n",
       "         0.00111372, 0.00109794, 0.0010603 , 0.0011109 , 0.00102228,\n",
       "         0.00109847, 0.00101778, 0.00106484, 0.00109073, 0.00103064,\n",
       "         0.00117283, 0.00108238, 0.00109848, 0.00104651, 0.00113102,\n",
       "         0.00110531, 0.00108574, 0.00100954, 0.00107669, 0.00109122,\n",
       "         0.00106309, 0.0010681 , 0.00109037, 0.00108162, 0.00105083,\n",
       "         0.00104869, 0.0010686 , 0.00113737, 0.00101562, 0.00110385,\n",
       "         0.00109703, 0.00108219, 0.00108162, 0.00104332, 0.00111996,\n",
       "         0.00108675, 0.00107851, 0.0010534 , 0.0010527 , 0.00112513,\n",
       "         0.00105054, 0.00108331, 0.00109638, 0.00110067, 0.00112393,\n",
       "         0.00106662, 0.00109805, 0.00109626, 0.00106779, 0.0010822 ,\n",
       "         0.00106282, 0.00110158, 0.00102542, 0.00110699, 0.00110136,\n",
       "         0.00106178, 0.00112917, 0.00108011, 0.00101964, 0.00108093,\n",
       "         0.00109948, 0.0011263 , 0.00106193, 0.00111825, 0.00107274,\n",
       "         0.00105793, 0.00106098, 0.00107774, 0.00107785, 0.00113774,\n",
       "         0.00106409, 0.001012  , 0.00102816, 0.00112419, 0.00104898,\n",
       "         0.00105823, 0.00105551, 0.00106602, 0.00109359, 0.00107212,\n",
       "         0.00100337, 0.00112932, 0.00109088, 0.00109544, 0.00108589,\n",
       "         0.00103259, 0.00110754, 0.0010514 , 0.00112761, 0.00108408,\n",
       "         0.00106863, 0.00103252, 0.00105177, 0.00109752, 0.00113151,\n",
       "         0.00107573, 0.00103067, 0.00111384, 0.00105465, 0.00107716,\n",
       "         0.00111226, 0.0011154 , 0.00109312, 0.00111054, 0.00106576,\n",
       "         0.00104575, 0.00112852, 0.00108154, 0.00104718, 0.00109987,\n",
       "         0.00111524, 0.00111428, 0.00107196, 0.00112302, 0.00111579,\n",
       "         0.00114011, 0.00102093, 0.00103561, 0.00105205, 0.00115243,\n",
       "         0.00104246, 0.00108694, 0.00102572, 0.00114406, 0.00106218,\n",
       "         0.00111103, 0.00106384, 0.00103395, 0.001072  , 0.00108862,\n",
       "         0.0011127 , 0.00109534, 0.00104643, 0.00102612, 0.00114777,\n",
       "         0.00115335, 0.00107294, 0.00113191, 0.00103495, 0.00108721,\n",
       "         0.00111585, 0.00106701, 0.00104021, 0.00104725, 0.00106165,\n",
       "         0.00105727, 0.00103051, 0.00108089, 0.00098555, 0.00102544,\n",
       "         0.00106015, 0.00100947, 0.00107377, 0.00107374, 0.00114892,\n",
       "         0.00101606, 0.0011474 , 0.00110498, 0.00111502, 0.00108261,\n",
       "         0.00105743, 0.00115727, 0.00108063, 0.00106084, 0.00108294,\n",
       "         0.00102444, 0.00118277, 0.00105976, 0.00106181, 0.00109522,\n",
       "         0.00110966, 0.00106305, 0.00108695, 0.00109857, 0.00114394,\n",
       "         0.00106676, 0.00106946, 0.00107751, 0.00110279, 0.00102761,\n",
       "         0.0011202 , 0.00113709, 0.00109397, 0.00108987, 0.00108946,\n",
       "         0.00110172, 0.00113308, 0.001107  , 0.00109142, 0.00110045,\n",
       "         0.00107612, 0.00104129, 0.00110649, 0.00106829, 0.00115999,\n",
       "         0.00107513, 0.00107508, 0.00115817, 0.00107134, 0.00102842,\n",
       "         0.00108133, 0.00107725, 0.00108868, 0.00103625, 0.0010156 ,\n",
       "         0.00103454, 0.00115766, 0.00104557, 0.00107511, 0.00104235,\n",
       "         0.00105568, 0.00116177, 0.00103748, 0.00113711, 0.00111447,\n",
       "         0.00111001, 0.0011097 , 0.0011092 , 0.00111349, 0.00108569,\n",
       "         0.00106977, 0.00106342, 0.00110458, 0.00105247, 0.00106488,\n",
       "         0.00104747, 0.0010634 , 0.00104179, 0.00106348, 0.00110232,\n",
       "         0.00106513, 0.00108597, 0.00108021, 0.00111839, 0.00105702,\n",
       "         0.00105044, 0.00107519, 0.00106586, 0.00109126, 0.0010622 ,\n",
       "         0.00103895, 0.00106098, 0.00108704, 0.00108352, 0.00108049,\n",
       "         0.00103633, 0.00108703, 0.00110134, 0.0010127 , 0.00113083,\n",
       "         0.00106754, 0.00107567, 0.00113306, 0.00110394, 0.00105025,\n",
       "         0.00106047, 0.00106799, 0.00105105, 0.00102625, 0.0010809 ,\n",
       "         0.0010249 , 0.00110399, 0.00106643, 0.00110789, 0.00113326,\n",
       "         0.0010706 , 0.00110707, 0.00108184, 0.00101427, 0.0011149 ,\n",
       "         0.00105822, 0.00108261, 0.00114997, 0.00105416, 0.00108795,\n",
       "         0.00101055, 0.00112558, 0.0010808 , 0.00108696, 0.00105653,\n",
       "         0.00105716, 0.00110036, 0.00112172, 0.00104563, 0.00115727,\n",
       "         0.00101095, 0.00100177, 0.00117987, 0.00101638, 0.00104476,\n",
       "         0.00109248, 0.00103779, 0.0010498 , 0.00117053, 0.00106512,\n",
       "         0.0010209 , 0.00105774, 0.00111225, 0.00106269, 0.00103406,\n",
       "         0.00108836, 0.00108372, 0.00109712, 0.00110964, 0.00112277,\n",
       "         0.00111168, 0.00110709, 0.00100639, 0.00108902, 0.00108259,\n",
       "         0.00108156, 0.00106234, 0.00111878, 0.00111899, 0.00106832,\n",
       "         0.00113969, 0.00110201, 0.0010237 , 0.0011294 , 0.00113468,\n",
       "         0.00105739, 0.00107909, 0.00107912, 0.00106344, 0.00106013,\n",
       "         0.00107628, 0.00108416, 0.00103861, 0.00105503, 0.00101267,\n",
       "         0.00099816, 0.00105003, 0.00111716, 0.00112529, 0.00106148,\n",
       "         0.0010873 , 0.00102864, 0.00110912, 0.00108158, 0.00105553,\n",
       "         0.00109911, 0.00105143, 0.00110997, 0.00108281, 0.00104129,\n",
       "         0.00113557, 0.0010228 , 0.00107332, 0.00104044, 0.00110397,\n",
       "         0.00105694, 0.00110654, 0.0010717 , 0.00109369, 0.00108387,\n",
       "         0.00109945, 0.00106902, 0.00107032, 0.00106858, 0.00107241,\n",
       "         0.00110714, 0.00105125, 0.00114756, 0.00117927, 0.0010843 ,\n",
       "         0.00105806, 0.00105024, 0.00113197, 0.00116183, 0.00101991,\n",
       "         0.00110434, 0.00113354, 0.00113478, 0.00107824, 0.00108511,\n",
       "         0.00103115, 0.0011054 , 0.00108044, 0.00111941, 0.00104028,\n",
       "         0.00106671, 0.00103709, 0.00103737, 0.00106822, 0.00109433,\n",
       "         0.00109577, 0.00107314, 0.00107155, 0.00104601, 0.00106761,\n",
       "         0.00109992, 0.00108308, 0.00103918, 0.00105778, 0.00108896,\n",
       "         0.00103146, 0.00103338, 0.00106381, 0.00105007, 0.00101066,\n",
       "         0.00106985, 0.00107375, 0.00110517, 0.00106545, 0.00108006,\n",
       "         0.0011315 , 0.00106242, 0.00107131, 0.0010952 , 0.0011078 ,\n",
       "         0.00100934, 0.00107999, 0.00107495, 0.00100287, 0.00108438,\n",
       "         0.00107476, 0.00107327, 0.00109525, 0.00112254, 0.00109105,\n",
       "         0.00103132, 0.00105056, 0.00109283, 0.00108167, 0.00109357,\n",
       "         0.00104298, 0.00106825, 0.00105623, 0.00111604, 0.00108295,\n",
       "         0.00107573, 0.00103105, 0.00113578, 0.00104999, 0.00108816,\n",
       "         0.00105748, 0.00108713, 0.00102924, 0.00112718, 0.00111145,\n",
       "         0.00112904, 0.0011186 , 0.00109966, 0.0010689 , 0.0010675 ,\n",
       "         0.00105702, 0.00105358, 0.00113273, 0.00102324, 0.00104465,\n",
       "         0.00110109, 0.00107337, 0.00105323, 0.00111201, 0.00111926,\n",
       "         0.00109609, 0.00112712, 0.00108677, 0.0010773 , 0.00112792,\n",
       "         0.00105719, 0.00103744, 0.00112526, 0.0010633 , 0.00110368,\n",
       "         0.00106357, 0.00107495, 0.00116301, 0.00104555, 0.00106552,\n",
       "         0.00106924, 0.00104023, 0.00106812, 0.00110547, 0.00110164]],\n",
       "       dtype=float32), array([[0.51089054]], dtype=float32)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "model.predict(np.array([trainX[0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"gz_dev.model.new\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
