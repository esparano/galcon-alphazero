{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "config = tf.ConfigProto(\n",
    "    gpu_options=tf.GPUOptions(per_process_gpu_memory_fraction=0.3)\n",
    "    # device_count = {'GPU': 1}\n",
    ")\n",
    "config.gpu_options.allow_growth = True\n",
    "session = tf.Session(config=config)\n",
    "set_session(session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "model = load_model(\"gz_dev.model\")\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading 1 training files\n",
      "[[ 0.49        0.16        0.47        0.          0.          1.\n",
      "  -2.15       -0.          0.          0.         -0.          0.\n",
      "   0.         -0.        ]\n",
      " [ 0.17        0.12        0.19        0.          0.          1.\n",
      "  -1.65        0.57        0.          0.         -0.          0.\n",
      "   0.         -0.        ]\n",
      " [ 0.79        0.21        0.06        0.          0.          1.\n",
      "  -1.24        1.06        0.          0.         -0.          0.\n",
      "   0.         -0.        ]\n",
      " [ 0.63        0.18        0.23        0.          0.          1.\n",
      "  -1.18       -0.22        0.          0.         -0.          0.\n",
      "   0.         -0.        ]\n",
      " [ 0.61        0.18        0.12        0.          0.          1.\n",
      "  -1.06        0.54        0.2        -0.83        0.3515      0.\n",
      "   0.         -0.        ]\n",
      " [ 0.35        0.14        0.3         0.          0.          1.\n",
      "  -0.82        1.37        0.          0.         -0.          0.\n",
      "   0.         -0.        ]\n",
      " [ 0.95        0.23        0.09        0.          0.          1.\n",
      "  -0.65       -0.01        0.05       -0.91        0.18        0.\n",
      "   0.         -0.        ]\n",
      " [ 0.59        0.18        0.          1.          0.          0.\n",
      "  -0.6         0.79        0.          0.         -0.          0.\n",
      "   0.         -0.        ]\n",
      " [ 0.93        0.23        0.02        0.          0.          1.\n",
      "  -0.34       -1.28        0.          0.         -0.          0.03\n",
      "  -0.31       -0.96      ]\n",
      " [ 1.          0.24        0.27        1.          0.          0.\n",
      "  -0.28        1.28        0.          0.         -0.          0.\n",
      "   0.         -0.        ]\n",
      " [ 0.16        0.12        0.16        0.          0.          1.\n",
      "  -0.15       -0.13        0.          0.         -0.          0.91\n",
      "   0.15120879 -0.46428571]\n",
      " [ 0.16        0.12        0.21        0.          0.          1.\n",
      "   0.15        0.13        0.25       -0.06        0.43        0.\n",
      "   0.         -0.        ]\n",
      " [ 1.          0.24        0.01        0.          1.          0.\n",
      "   0.28       -1.28        0.          0.         -0.          0.03\n",
      "   0.09       -0.88666667]\n",
      " [ 0.93        0.23        0.02        0.          0.          1.\n",
      "   0.34        1.28        0.          0.         -0.          0.\n",
      "   0.         -0.        ]\n",
      " [ 0.59        0.18        0.25        0.          0.          1.\n",
      "   0.6        -0.79        0.          0.         -0.          0.\n",
      "   0.         -0.        ]\n",
      " [ 0.95        0.23        0.09        0.          0.          1.\n",
      "   0.65        0.01        0.          0.         -0.          0.\n",
      "   0.         -0.        ]\n",
      " [ 0.35        0.14        0.3         0.          0.          1.\n",
      "   0.82       -1.37        0.          0.         -0.          0.\n",
      "   0.         -0.        ]\n",
      " [ 0.61        0.18        0.12        0.          0.          1.\n",
      "   1.06       -0.54        0.          0.         -0.          0.\n",
      "   0.         -0.        ]\n",
      " [ 0.63        0.18        0.24        0.          0.          1.\n",
      "   1.18        0.22        0.          0.         -0.          0.\n",
      "   0.         -0.        ]\n",
      " [ 0.79        0.21        0.06        0.          0.          1.\n",
      "   1.24       -1.06        0.          0.         -0.          0.\n",
      "   0.         -0.        ]\n",
      " [ 0.17        0.12        0.19        0.          0.          1.\n",
      "   1.65       -0.57        0.          0.         -0.          0.\n",
      "   0.         -0.        ]\n",
      " [ 0.49        0.16        0.47        0.          0.          1.\n",
      "   2.15       -0.          0.          0.         -0.          0.\n",
      "   0.         -0.        ]]\n",
      "[[ 0.49        0.16        0.47        0.          0.          1.\n",
      "  -2.15        0.          0.          0.          0.          0.\n",
      "   0.          0.        ]\n",
      " [ 0.17        0.12        0.19        0.          0.          1.\n",
      "  -1.65       -0.57        0.          0.          0.          0.\n",
      "   0.          0.        ]\n",
      " [ 0.79        0.21        0.06        0.          0.          1.\n",
      "  -1.24       -1.06        0.          0.          0.          0.\n",
      "   0.          0.        ]\n",
      " [ 0.63        0.18        0.23        0.          0.          1.\n",
      "  -1.18        0.22        0.          0.          0.          0.\n",
      "   0.          0.        ]\n",
      " [ 0.61        0.18        0.12        0.          0.          1.\n",
      "  -1.06       -0.54        0.2        -0.83       -0.3515      0.\n",
      "   0.          0.        ]\n",
      " [ 0.35        0.14        0.3         0.          0.          1.\n",
      "  -0.82       -1.37        0.          0.          0.          0.\n",
      "   0.          0.        ]\n",
      " [ 0.95        0.23        0.09        0.          0.          1.\n",
      "  -0.65        0.01        0.05       -0.91       -0.18        0.\n",
      "   0.          0.        ]\n",
      " [ 0.59        0.18        0.          1.          0.          0.\n",
      "  -0.6        -0.79        0.          0.          0.          0.\n",
      "   0.          0.        ]\n",
      " [ 0.93        0.23        0.02        0.          0.          1.\n",
      "  -0.34        1.28        0.          0.          0.          0.03\n",
      "  -0.31        0.96      ]\n",
      " [ 1.          0.24        0.27        1.          0.          0.\n",
      "  -0.28       -1.28        0.          0.          0.          0.\n",
      "   0.          0.        ]\n",
      " [ 0.16        0.12        0.16        0.          0.          1.\n",
      "  -0.15        0.13        0.          0.          0.          0.91\n",
      "   0.15120879  0.46428571]\n",
      " [ 0.16        0.12        0.21        0.          0.          1.\n",
      "   0.15       -0.13        0.25       -0.06       -0.43        0.\n",
      "   0.          0.        ]\n",
      " [ 1.          0.24        0.01        0.          1.          0.\n",
      "   0.28        1.28        0.          0.          0.          0.03\n",
      "   0.09        0.88666667]\n",
      " [ 0.93        0.23        0.02        0.          0.          1.\n",
      "   0.34       -1.28        0.          0.          0.          0.\n",
      "   0.          0.        ]\n",
      " [ 0.59        0.18        0.25        0.          0.          1.\n",
      "   0.6         0.79        0.          0.          0.          0.\n",
      "   0.          0.        ]\n",
      " [ 0.95        0.23        0.09        0.          0.          1.\n",
      "   0.65       -0.01        0.          0.          0.          0.\n",
      "   0.          0.        ]\n",
      " [ 0.35        0.14        0.3         0.          0.          1.\n",
      "   0.82        1.37        0.          0.          0.          0.\n",
      "   0.          0.        ]\n",
      " [ 0.61        0.18        0.12        0.          0.          1.\n",
      "   1.06        0.54        0.          0.          0.          0.\n",
      "   0.          0.        ]\n",
      " [ 0.63        0.18        0.24        0.          0.          1.\n",
      "   1.18       -0.22        0.          0.          0.          0.\n",
      "   0.          0.        ]\n",
      " [ 0.79        0.21        0.06        0.          0.          1.\n",
      "   1.24        1.06        0.          0.          0.          0.\n",
      "   0.          0.        ]\n",
      " [ 0.17        0.12        0.19        0.          0.          1.\n",
      "   1.65        0.57        0.          0.          0.          0.\n",
      "   0.          0.        ]\n",
      " [ 0.49        0.16        0.47        0.          0.          1.\n",
      "   2.15        0.          0.          0.          0.          0.\n",
      "   0.          0.        ]]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "from trainingHelper import TrainingHelper\n",
    "from trainingGame import loadTrainingGame\n",
    "import nnHelper\n",
    "import numpy as np\n",
    "\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "filepath = \"D:/GalconZero/Games/iteration_0\"\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "def getTrainingDataForNumGames(num):\n",
    "    trainX = np.empty((0, 22, 14))\n",
    "    trainY = [np.empty((0, 925)), np.empty((0,))]\n",
    "\n",
    "    for _ in [1]:\n",
    "        trainingGame = loadTrainingGame(filepath + \"/game_1562210791.2397857_3.pickle\")\n",
    "        helper = TrainingHelper(trainingGame)    \n",
    "        trainX = np.concatenate((trainX, helper.getTrainX()))\n",
    "        [policy, value] = helper.getTrainY()\n",
    "        trainY = [np.concatenate((trainY[0], policy)), np.concatenate((trainY[1], value))]\n",
    "\n",
    "    print(\"Finished loading {} training files\".format(num))\n",
    "    return trainX, trainY\n",
    "\n",
    "trainX, trainY = getTrainingDataForNumGames(1)\n",
    "\n",
    "def getYAxisFlippedTrainingData(data):\n",
    "    data = data.copy()\n",
    "    for sample in data:\n",
    "        for row in sample:\n",
    "            row[7] *= -1\n",
    "            row[10] *= -1\n",
    "            row[13] *= -1\n",
    "    return data\n",
    "\n",
    "# this involves reversing ordering of each sample in trainX, then mapping indices somehow to fix trainY, both sends and redirects\n",
    "def getXAxisFlippedTrainingData(data):\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "historyPolicyValAcc = []\n",
    "historyValueValAcc = []\n",
    "\n",
    "historyPolicyValLoss = []\n",
    "historyValueValLoss = []\n",
    "historyValLoss = []\n",
    "\n",
    "def doTrain(num_loops):\n",
    "    for i in range(num_loops):\n",
    "        history = trainingLoop()\n",
    "\n",
    "        historyPolicyValAcc.extend(history.history['val_policy_acc'])\n",
    "        historyValueValAcc.extend(history.history['val_value_acc'])\n",
    "        \n",
    "        historyPolicyValLoss.extend(history.history['val_policy_loss'])\n",
    "        historyValueValLoss.extend(history.history['val_value_loss'])\n",
    "        historyValLoss.extend(history.history['val_loss'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doTrain(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "    \n",
    "plt.figure(1, figsize=(18, 18))  \n",
    "   \n",
    "# summarize history for accuracy  \n",
    "   \n",
    "plt.subplot(211)  \n",
    "# plt.plot(history.history['policy_acc'])  \n",
    "plt.plot(historyPolicyValAcc)  \n",
    "# plt.plot(history.history['value_acc'])  \n",
    "plt.plot(historyValueValAcc)  \n",
    "plt.title('model accuracy')  \n",
    "plt.ylabel('accuracy')  \n",
    "plt.xlabel('epoch')  \n",
    "# plt.legend(['policy_categorical_accuracy', 'val_policy_categorical_accuracy', 'value_categorical_accuracy', 'val_value_categorical_accuracy'], loc='upper left')  \n",
    "plt.legend(['Validation Policy Accuracy','Validation Value Accuracy'], loc='upper left')  \n",
    "   \n",
    "# summarize history for loss  \n",
    "   \n",
    "plt.subplot(212)  \n",
    "# plt.plot(history.history['policy_loss'])  \n",
    "plt.plot(historyPolicyValLoss)  \n",
    "# plt.plot(history.history['value_loss'])  \n",
    "plt.plot(historyValueValLoss)  \n",
    "# plt.plot(history.history['loss'])  \n",
    "plt.plot(historyValLoss)  \n",
    "plt.title('model loss')  \n",
    "plt.ylabel('loss')  \n",
    "plt.xlabel('epoch')  \n",
    "# plt.legend(['policy_loss', 'val_policy_loss', 'value_loss','val_value_loss','loss','val_loss',], loc='upper left')  \n",
    "plt.legend(['val_policy_loss','val_value_loss','val_loss'], loc='upper left')  \n",
    "\n",
    "plt.show()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"gz_dev.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
